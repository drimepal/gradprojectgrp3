{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender name data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import pickle\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norwegian gender names\n",
    "df = pd.read_excel('norwegian_female_names.xlsx')\n",
    "df['origin'] = \"norwegian\"\n",
    "df_norwegian_female_names = df[['name', 'origin']]\n",
    "\n",
    "df = pd.read_excel('norwegian_male_names.xlsx')\n",
    "df = df.groupby('name')\n",
    "df = pd.DataFrame(df, columns = ['name', 'sex'])\n",
    "df['origin'] = \"norwegian\"\n",
    "df_norwegian_male_names = df[['name', 'origin']]\n",
    "\n",
    "df_norway = pd.concat([df_norwegian_female_names, df_norwegian_male_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding swedish names:\n",
    "list_of_swed_male_names = ['Hugo', 'Matteo', 'Walter', 'Harry', 'Vincent', 'Charlie', 'Arvid', 'Frans', 'Sam', 'Louie', 'Otto', 'Malte', 'Elton', 'Ebbe', 'Love', 'Olle', 'Melvin', 'Albin', 'Jack', 'Frank', 'Tage', 'Colin', 'Noel', 'Sixten', 'Viggo', 'Vidar', 'Melker', 'Loke', 'Ivar', 'Milo', 'Sigge', 'Alvin', 'Kian', 'Nicholas', 'Anton', 'Milton', 'Vilgot', 'Wilmer', 'Wilhelm', 'Elis', 'Dante', 'Ture', 'Elvin', 'Folke', 'Algot', 'Hjalmar', 'Vide', 'Alvar', 'Ted', 'Alve', 'Joel', 'Björn', 'Otis', 'Svante']\n",
    "list_of_swed_female_names = ['Alice', 'Elsa', 'Wilma', 'Freja', 'Vera', 'Ines', 'Ester', 'Ellen', 'Molly', 'Lo', 'Nova', 'Mila', 'Lova', 'Juni', 'Isabelle', 'Edith', 'Nellie', 'Leia', 'Meja', 'Livia', 'Rut', 'Tyra', 'Cleo', 'Majken', 'Lovisa', 'Sally', 'Elin', 'Lykke', 'Hailey', 'Bonnie', 'Hilma', 'Melina', 'Mira', 'Märta', 'Hilda', 'Melissa', 'Svea', 'Zoe', 'Ilse', 'Stina', 'Joline', 'Lovis', 'Tilde', 'Elsie', 'Siri', 'Elina', 'Maryam', 'Belle', 'Felicia', 'Matilda', 'Bianca', 'Moa', 'Ayla', 'Idun', 'Millie']\n",
    "\n",
    "df_swed_m = pd.DataFrame(list_of_swed_male_names, columns = ['name'])\n",
    "df_swed_m['origin'] = \"swedish\"\n",
    "df_swed_f = pd.DataFrame(list_of_swed_female_names, columns = ['name'])\n",
    "df_swed_f['origin'] = \"swedish\"\n",
    "\n",
    "df_sweden = pd.concat([df_swed_f, df_swed_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Islamic gender names\n",
    "islamic_female_name = []\n",
    "with open('islamske_kvinne_navn.txt','r') as file:   \n",
    "    # reading each line    \n",
    "    for line in file:   \n",
    "        # reading each word        \n",
    "        for name in line.split():   \n",
    "            # displaying the words\n",
    "            name = name.strip(\",\")           \n",
    "            islamic_female_name.append(name)\n",
    "df_islamic_female_name = pd.DataFrame(islamic_female_name, columns=[\"name\"])\n",
    "df_islamic_female_name['origin'] = \"arabic\"\n",
    "\n",
    "islamic_male_name = []\n",
    "with open('islamske_mannenavn_navn.txt','r') as file:   \n",
    "    # reading each line    \n",
    "    for line in file:   \n",
    "        # reading each word        \n",
    "        for name in line.split():   \n",
    "            # displaying the words\n",
    "            name = name.strip(\",\")           \n",
    "            islamic_male_name.append(name)\n",
    "df_islamic_male_name = pd.DataFrame(islamic_male_name, columns=[\"name\"])\n",
    "df_islamic_male_name['origin'] = \"arabic\"\n",
    "\n",
    "df_arabic = pd.concat([df_islamic_male_name, df_islamic_female_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Islamic gender names\n",
    "universal_name_origin = []\n",
    "first_line = True\n",
    "with open('name_origin.csv','r') as file:   \n",
    "    for line in file:\n",
    "        if first_line != True:\n",
    "\n",
    "            row = []\n",
    "            for item in line.split(\",\"):   \n",
    "                item = item.strip(\" \")\n",
    "                item = item.strip(\"\\n\")\n",
    "                row.append(item)\n",
    "            universal_name_origin.append(row)\n",
    "        else:\n",
    "            first_line = False\n",
    "df_universal_name = pd.DataFrame(universal_name_origin, columns=[\"index\", \"name\", \"origin\"])\n",
    "df_universal_name = df_universal_name[[\"name\", \"origin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining into a df_gender_name\n",
    "df = pd.concat([df_universal_name,\n",
    "                df_norway,\n",
    "                df_sweden,\n",
    "                df_arabic\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yiddish', 'gaelic', 'african', 'irish', 'hungarian', 'german',\n",
       "       'swedish', 'japanese', 'italian', 'american', 'hawaiian', 'greek',\n",
       "       'polynesian', 'scandinavian', 'spanish', 'celtic', 'old-english',\n",
       "       'korean', 'sanskrit', 'african-american', 'hebrew', 'norse',\n",
       "       'chinese', 'finnish', 'persian', 'scottish', 'slavic', 'english',\n",
       "       'old-norse', 'dutch', 'armenian', 'welsh', 'polish', 'teutonic',\n",
       "       'russian', 'egyptian', 'arabic', 'swahili', 'native-american',\n",
       "       'old-french', 'french', 'middle-english', 'latin', 'vietnamese',\n",
       "       'danish', 'hindi', 'old-german', 'turkish', 'indian',\n",
       "       'czechoslovakian', 'norwegian'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"origin\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scandinavian = ['scandinavian' ,'norwegian', 'swedish', 'danish', 'norse', 'old-norse']\n",
    "sami_finish = ['finnish']\n",
    "african = ['swahili', 'african']\n",
    "arabic = ['turkish', 'arabic', 'persian', 'egyptian']\n",
    "western_european = ['teutonic','dutch', 'scottish','english','spanish','american','old-english','italian', 'irish','old-french', 'french', 'middle-english', 'latin', 'old-german', 'german']\n",
    "celtic = ['gaelic', 'celtic', 'welsh']\n",
    "eastern_european = ['polish', 'czechoslovakian', 'slavic', 'russian', 'armenian']\n",
    "asian = ['hindi', 'indian', 'vietnamese', 'japanese', 'korean', 'chinese', 'sanskrit']\n",
    "other_european = ['hungarian', 'greek']\n",
    "pacific_islander = ['hawaiian', 'polynesian']\n",
    "non_european_american = ['african-american','native-american']\n",
    "jewish = ['yiddish', 'hebrew']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_classefier(origin):\n",
    "    if origin in scandinavian:\n",
    "        return 'scandinavian'\n",
    "    elif origin in sami_finish:\n",
    "        return 'sami_finish'\n",
    "    elif origin in african:\n",
    "        return 'african'\n",
    "    elif origin in arabic:\n",
    "        return 'arabic'\n",
    "    elif origin in western_european:\n",
    "        return 'western_european'\n",
    "    elif origin in celtic:\n",
    "        return 'celtic'\n",
    "    elif origin in eastern_european:\n",
    "        return 'eastern_european'\n",
    "    elif origin in asian:\n",
    "        return 'asian'\n",
    "    elif origin in other_european:\n",
    "        return 'other_european'\n",
    "    elif origin in pacific_islander:\n",
    "        return 'pacific_islander'\n",
    "    elif origin in non_european_american:\n",
    "        return 'non_european_american'\n",
    "    elif origin in jewish:\n",
    "        return 'jewish'\n",
    "    else:\n",
    "        print(\"NOT FOUND\")\n",
    "        print(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nordic = ['finnish','scandinavian' ,'norwegian', 'swedish', 'danish', 'norse', 'old-norse']\n",
    "african_middel_eastern = ['swahili', 'african','turkish', 'arabic', 'persian', 'egyptian']\n",
    "western_european = ['hungarian', 'greek','gaelic', 'celtic', 'welsh','teutonic','dutch', 'scottish','english','spanish','american','old-english','italian', 'irish','old-french', 'french', 'middle-english', 'latin', 'old-german', 'german']\n",
    "eastern_european = ['polish', 'czechoslovakian', 'slavic', 'russian', 'armenian']\n",
    "asian = ['hindi', 'indian', 'vietnamese', 'japanese', 'korean', 'chinese', 'sanskrit']\n",
    "other = ['yiddish', 'hebrew', 'african-american','native-american', 'hawaiian', 'polynesian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_region_classefier(origin):\n",
    "    if origin in nordic:\n",
    "        return 'nordic'\n",
    "    elif origin in african_middel_eastern:\n",
    "        return 'african_middel_eastern'\n",
    "    elif origin in western_european:\n",
    "        return 'western_european'\n",
    "    elif origin in eastern_european:\n",
    "        return 'eastern_european'\n",
    "    elif origin in asian:\n",
    "        return 'asian'\n",
    "    elif origin in other:\n",
    "        return 'other'\n",
    "    else:\n",
    "        print(\"NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scandinavian = ['scandinavian' ,'norwegian', 'swedish', 'danish', 'norse', 'old-norse']\n",
    "african_arabic = ['swahili', 'african', 'turkish', 'arabic', 'persian', 'egyptian', 'armenian']\n",
    "european = ['hungarian', 'greek','polish', 'czechoslovakian', 'slavic', 'russian', 'gaelic', 'celtic', 'welsh', 'finnish', 'teutonic','dutch', 'scottish','english','spanish','american','old-english','italian', 'irish','old-french', 'french', 'middle-english', 'latin', 'old-german', 'german']\n",
    "asian = ['hindi', 'indian', 'vietnamese', 'japanese', 'korean', 'chinese', 'sanskrit']\n",
    "other = ['hawaiian', 'polynesian', 'african-american','native-american', 'yiddish', 'hebrew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_region_classefier(origin):\n",
    "    if origin in scandinavian:\n",
    "        return 'scandinavian'\n",
    "    elif origin in african_arabic:\n",
    "        return 'african_arabic'\n",
    "    elif origin in european:\n",
    "        return 'european'\n",
    "    elif origin in asian:\n",
    "        return 'asian'\n",
    "    elif origin in other:\n",
    "        return 'other'\n",
    "    else:\n",
    "        print(\"NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['region'] = df['origin'].map(region_classefier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['super_region'] = df['origin'].map(super_region_classefier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['master_region'] = df['origin'].map(master_region_classefier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data to two lists:\n",
    "# And last minute clean up, removal of \" \" characters.\n",
    "name_list = df.name.tolist()\n",
    "temp_name_list = []\n",
    "X = []\n",
    "for name in name_list:\n",
    "    X.append(name.strip())\n",
    "y1 = df.origin.tolist()\n",
    "y2 = df.region.tolist()\n",
    "y3 = df.super_region.tolist()\n",
    "y4 = df.master_region.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_name(X):\n",
    "    for idx, name in enumerate(X):\n",
    "        for char in name:\n",
    "            if char == \" \" or char == \"½\" or char == \"Ã\":\n",
    "                return(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5569\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing more to remove\n"
     ]
    }
   ],
   "source": [
    "#index = remove_unwanted_name(X)\n",
    "while True:\n",
    "    try:\n",
    "        index = remove_unwanted_name(X)\n",
    "        X.pop(index)\n",
    "        y1.pop(index)\n",
    "        y2.pop(index)\n",
    "        y3.pop(index)\n",
    "        y4.pop(index)\n",
    "    except:\n",
    "        print(\"Nothing more to remove\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5528\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in X:\n",
    "    for char in name:\n",
    "        if char.lower() not in ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','æ','ø','å','à','á','ä','ö','ü','í','é',\"'\",'-',]:\n",
    "            print(char)\n",
    "            print(name)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [X,y1,y2,y3,y4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data\n",
    "parent = path.abspath(path.join(\"gender_name_data_managment.ipynb\", \"../..\"))\n",
    "path = \"/processed/\"\n",
    "file_name = \"origin_data\"\n",
    "\n",
    "with open(f'{parent + path + file_name}.obj', 'wb') as f:\n",
    "\tpickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c2940c5258610f9a2b978227281dc0bf3c3e3ce460e55d30a45b10f147711e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
